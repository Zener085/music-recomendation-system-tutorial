{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Hello! Here you can find how to implement GNN and how to use it for RecSys.\n",
    "We will recommend music to users\n",
    "using [MAGNN architecture](https://arxiv.org/pdf/2002.01680.pdf) and [LastFM dataset](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.LastFM.html).\n",
    "Specifically, the model will be used for \"link prediction\" task by using collaborative filtering.\n",
    "\n",
    "First of all, let's import everything for the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:27:37.761995300Z",
     "start_time": "2023-12-12T15:27:24.563945300Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.datasets import LastFM\n",
    "\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:27:37.786172300Z",
     "start_time": "2023-12-12T15:27:37.761995300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x18f76c92490>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:27:37.843692800Z",
     "start_time": "2023-12-12T15:27:37.777194400Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_USERS = 1892\n",
    "NUM_MOVIES = 17632\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "num_user_features = 1\n",
    "num_movie_features = 1\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's extract the dataset and create a new feature for it. We should do it to increase the accuracy of the model in the future. This number represents the similarity or closeness between the two users.\n",
    "In collaborative filtering, similarity is often measured based on the users' preferences, behaviors, or interactions with items (in this case, probably music tracks on LastFM).\n",
    "The value between 0 and 1 indicates the degree of similarity, with 1 being a perfect match (users are very similar), and 0 being no similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:27:41.937638600Z",
     "start_time": "2023-12-12T15:27:37.828734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading initial dataset\n",
    "dataset = LastFM(\"data/raw/LastFM\")\n",
    "\n",
    "# Loading preprocessed data\n",
    "path = \"./data/raw/LastFM/raw/\"\n",
    "\n",
    "# Extract features from dataset \n",
    "user_sums = defaultdict(lambda: torch.tensor(0))\n",
    "user_counts = defaultdict(lambda: torch.tensor(0))\n",
    "with open(path + \"user_user(knn).dat\") as f:\n",
    "    for line in f:\n",
    "        u1, _, w = map(float, line.split())\n",
    "        u1 = int(u1)\n",
    "        user_sums[u1] = user_sums[u1] + w\n",
    "        user_counts[u1] += 1\n",
    "\n",
    "movie_sums = defaultdict(lambda: torch.tensor(0))\n",
    "movie_counts = defaultdict(lambda: torch.tensor(0))\n",
    "with open(path + \"artist_artist(knn).dat\") as f:\n",
    "    for line in f:\n",
    "        u1, _, w = map(float, line.split())\n",
    "        u1 = int(u1)\n",
    "        movie_sums[u1] = movie_sums[u1] + w\n",
    "        movie_counts[u1] += 1\n",
    "\n",
    "# Take average value\n",
    "user_features = {i: (user_sums[i] / user_counts[i]).clone().detach() for i in user_sums}\n",
    "movie_features = {i: (movie_sums[i] / movie_counts[i]).clone().detach() for i in movie_sums}\n",
    "\n",
    "user_features = defaultdict(lambda: torch.tensor(0), user_features)\n",
    "movie_features = defaultdict(lambda: torch.tensor(0), movie_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:27:41.953726200Z",
     "start_time": "2023-12-12T15:27:41.940631300Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we will preprocess data and then split it to train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:28:52.550996400Z",
     "start_time": "2023-12-12T15:27:57.080233600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1892/1892 [00:36<00:00, 52.09it/s]\n"
     ]
    }
   ],
   "source": [
    "edge_index = dataset[\"user\", \"to\", \"artist\"][\"edge_index\"]\n",
    "train_movie_metapaths = defaultdict(lambda: np.zeros(shape=(0, 3)))\n",
    "for i in tqdm(edge_index[0].unique()):\n",
    "    neighbors = torch.where((edge_index[0] == i).clone().detach(), edge_index, -1)[1].unique()[1:]\n",
    "    comb = torch.combinations(neighbors, with_replacement=True)\n",
    "    col = torch.ones(size=(comb.shape[0], 1)) * i\n",
    "    meta_paths = torch.hstack((comb[:, :1], col, comb[:, 1:]))\n",
    "    for path in meta_paths:\n",
    "        train_movie_metapaths[int(path[0])] = np.vstack((train_movie_metapaths[int(path[0])], path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:28:59.111369700Z",
     "start_time": "2023-12-12T15:28:52.552991Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1863/1863 [00:03<00:00, 478.03it/s]\n"
     ]
    }
   ],
   "source": [
    "edge_index = dataset[\"user\", \"to\", \"artist\"][\"test_pos_edge_index\"]\n",
    "test_movie_metapaths = defaultdict(lambda: np.zeros(shape=(0, 3)))\n",
    "for i in tqdm(edge_index[0].unique()):\n",
    "    neighbors = torch.where((edge_index[0] == i).clone().detach(), edge_index, -1)[1].unique()[1:]\n",
    "    comb = torch.combinations(neighbors, with_replacement=True)\n",
    "    col = torch.ones(size=(comb.shape[0], 1)) * i\n",
    "    meta_paths = torch.hstack((comb[:, :1], col, comb[:, 1:]))\n",
    "    for path in meta_paths:\n",
    "        test_movie_metapaths[int(path[0])] = np.vstack((test_movie_metapaths[int(path[0])], path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:30:27.845560100Z",
     "start_time": "2023-12-12T15:28:59.111369700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17632/17632 [00:59<00:00, 296.35it/s] \n"
     ]
    }
   ],
   "source": [
    "edge_index = dataset[\"artist\", \"to\", \"user\"][\"edge_index\"]\n",
    "train_user_metapaths = defaultdict(lambda: np.zeros(shape=(0, 3)))\n",
    "for i in tqdm(edge_index[0].unique()):\n",
    "    neighbors = torch.where((edge_index[0] == i).clone().detach(), edge_index, -1)[1].unique()[1:]\n",
    "    comb = torch.combinations(neighbors, with_replacement=True)\n",
    "    col = torch.ones(size=(comb.shape[0], 1)) * i\n",
    "    meta_paths = torch.hstack((comb[:, :1], col, comb[:, 1:]))\n",
    "    for path in meta_paths:\n",
    "        train_user_metapaths[int(path[0])] = np.vstack((train_user_metapaths[int(path[0])], path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:30:38.930458100Z",
     "start_time": "2023-12-12T15:30:27.848552500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4041/4041 [00:08<00:00, 488.62it/s] \n"
     ]
    }
   ],
   "source": [
    "edge_index = dataset[\"user\", \"to\", \"artist\"][\"test_pos_edge_index\"]\n",
    "test_user_metapaths = defaultdict(lambda: np.zeros(shape=(0, 3)))\n",
    "for i in tqdm(edge_index[1].unique()):\n",
    "    neighbors = torch.where((edge_index[1] == i).clone().detach(), edge_index, -1)[0].unique()[1:]\n",
    "    comb = torch.combinations(neighbors, with_replacement=True)\n",
    "    col = torch.ones(size=(comb.shape[0], 1)) * i\n",
    "    meta_paths = torch.hstack((comb[:, :1], col, comb[:, 1:]))\n",
    "    for path in meta_paths:\n",
    "        test_user_metapaths[int(path[0])] = np.vstack((test_user_metapaths[int(path[0])], path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:30:38.932452600Z",
     "start_time": "2023-12-12T15:30:38.915745100Z"
    }
   },
   "outputs": [],
   "source": [
    "neg_edge_index = dataset[\"user\", \"to\", \"artist\"][\"train_neg_edge_index\"]\n",
    "\n",
    "train_pos = dataset[\"user\", \"to\", \"artist\"][\"edge_index\"]\n",
    "train_neg = neg_edge_index[:, :train_pos.shape[1]]\n",
    "\n",
    "test_pos = dataset[\"user\", \"to\", \"artist\"][\"test_pos_edge_index\"]\n",
    "test_neg = dataset[\"user\", \"to\", \"artist\"][\"test_neg_edge_index\"][:, :test_pos.shape[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We also might need to do preprocess flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T11:58:27.032884800Z",
     "start_time": "2023-12-12T11:58:27.011031100Z"
    }
   },
   "outputs": [],
   "source": [
    "class UserMovieDataset(Dataset):\n",
    "    \"\"\"Dataset for MAGNN training and evaluation\"\"\"\n",
    "\n",
    "    def __init__(self, positives: torch.Tensor, user_meta_paths: defaultdict, movie_meta_paths: defaultdict,\n",
    "                 max_user_size: int, max_movie_size: int, negatives=None, benchmark: bool = False):\n",
    "        self.benchmark = benchmark\n",
    "        self.max_user_size = max_user_size\n",
    "        self.max_movie_size = max_movie_size\n",
    "\n",
    "        self.user_meta_paths = user_meta_paths.copy()\n",
    "        self.movie_meta_paths = movie_meta_paths.copy()\n",
    "        self.merge_meta_paths()\n",
    "\n",
    "        self.pos = positives.numpy()\n",
    "        self.neg = negatives.numpy()\n",
    "\n",
    "        self.data = torch.tensor(np.vstack([self.pos.T, self.neg.T]))\n",
    "        self.labels = torch.vstack([torch.ones((len(self.pos.T), 1)), torch.zeros((len(self.neg.T), 1))])\n",
    "\n",
    "    def merge_meta_paths(self):\n",
    "        for key, val in tqdm(self.user_meta_paths.items(), desc=\"Extracting user meta-paths\"):\n",
    "            if len(self.user_meta_paths[key]) > 0:\n",
    "                self.user_meta_paths[key] = [torch.vstack([user_features[i] for i in val[:, 0]])[:self.max_user_size],\n",
    "                                             torch.vstack([movie_features[i] for i in val[:, 1]])[:self.max_user_size],\n",
    "                                             torch.vstack([user_features[i] for i in val[:, 2]])[:self.max_user_size]]\n",
    "                while len(self.user_meta_paths[key][0]) < self.max_user_size:\n",
    "                    self.user_meta_paths[key][0] = torch.vstack(\n",
    "                        [self.user_meta_paths[key][0], torch.zeros_like(self.user_meta_paths[key][0][0])])\n",
    "                    self.user_meta_paths[key][1] = torch.vstack(\n",
    "                        [self.user_meta_paths[key][1], torch.zeros_like(self.user_meta_paths[key][1][0])])\n",
    "                    self.user_meta_paths[key][2] = torch.vstack(\n",
    "                        [self.user_meta_paths[key][2], torch.zeros_like(self.user_meta_paths[key][2][0])])\n",
    "\n",
    "        for key, val in tqdm(self.movie_meta_paths.items(), desc=\"Extracting movie meta-paths\"):\n",
    "            if len(self.movie_meta_paths[key]) > 0:\n",
    "                self.movie_meta_paths[key] = [\n",
    "                    torch.vstack([movie_features[i] for i in val[:, 0]])[:self.max_movie_size],\n",
    "                    torch.vstack([user_features[i] for i in val[:, 1]])[:self.max_movie_size],\n",
    "                    torch.vstack([movie_features[i] for i in val[:, 2]])[:self.max_movie_size]]\n",
    "                while len(self.movie_meta_paths[key][0]) < self.max_movie_size:\n",
    "                    self.movie_meta_paths[key][0] = torch.vstack(\n",
    "                        [self.movie_meta_paths[key][0], torch.zeros_like(self.movie_meta_paths[key][0][0])])\n",
    "                    self.movie_meta_paths[key][1] = torch.vstack(\n",
    "                        [self.movie_meta_paths[key][1], torch.zeros_like(self.movie_meta_paths[key][1][0])])\n",
    "                    self.movie_meta_paths[key][2] = torch.vstack(\n",
    "                        [self.movie_meta_paths[key][2], torch.zeros_like(self.movie_meta_paths[key][2][0])])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_id, movie_id = self.data[idx]\n",
    "\n",
    "        if len(self.user_meta_paths[user_id.item()]) == 0:\n",
    "            key = user_id.item()\n",
    "            self.user_meta_paths[key] = [torch.Tensor([0]), torch.Tensor([0]), torch.Tensor([0])]\n",
    "            while len(self.user_meta_paths[key][0]) < self.max_user_size:\n",
    "                self.user_meta_paths[key][0] = torch.vstack(\n",
    "                    [self.user_meta_paths[key][0], torch.zeros_like(self.user_meta_paths[key][0][0])])\n",
    "                self.user_meta_paths[key][1] = torch.vstack(\n",
    "                    [self.user_meta_paths[key][1], torch.zeros_like(self.user_meta_paths[key][1][0])])\n",
    "                self.user_meta_paths[key][2] = torch.vstack(\n",
    "                    [self.user_meta_paths[key][2], torch.zeros_like(self.user_meta_paths[key][2][0])])\n",
    "\n",
    "        if len(self.movie_meta_paths[movie_id.item()]) == 0:\n",
    "            key = movie_id.item()\n",
    "            self.movie_meta_paths[key] = [torch.Tensor([0]), torch.Tensor([0]), torch.Tensor([0])]\n",
    "            while len(self.movie_meta_paths[key][0]) < self.max_movie_size:\n",
    "                self.movie_meta_paths[key][0] = torch.vstack(\n",
    "                    [self.movie_meta_paths[key][0], torch.zeros_like(self.movie_meta_paths[key][0][0])])\n",
    "                self.movie_meta_paths[key][1] = torch.vstack(\n",
    "                    [self.movie_meta_paths[key][1], torch.zeros_like(self.movie_meta_paths[key][1][0])])\n",
    "                self.movie_meta_paths[key][2] = torch.vstack(\n",
    "                    [self.movie_meta_paths[key][2], torch.zeros_like(self.movie_meta_paths[key][2][0])])\n",
    "\n",
    "        return self.data[idx], self.labels[idx], *self.user_meta_paths[user_id.item()], *self.movie_meta_paths[\n",
    "            movie_id.item()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T12:00:11.192838200Z",
     "start_time": "2023-12-12T11:58:27.047659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting user meta-paths: 100%|██████████| 1892/1892 [00:34<00:00, 54.55it/s]\n",
      "Extracting movie meta-paths: 100%|██████████| 17632/17632 [00:38<00:00, 462.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting user meta-paths: 100%|██████████| 1863/1863 [00:04<00:00, 383.52it/s]\n",
      "Extracting movie meta-paths: 100%|██████████| 4041/4041 [00:03<00:00, 1149.22it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Dataset:\")\n",
    "train_dataset = UserMovieDataset(train_pos, train_user_metapaths, train_movie_metapaths, 905, 67, train_neg)\n",
    "print(\"Test Dataset:\")\n",
    "test_dataset = UserMovieDataset(test_pos, test_user_metapaths, test_movie_metapaths, 131, 28, test_neg, benchmark=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T12:00:11.225651700Z",
     "start_time": "2023-12-12T12:00:11.189996100Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So let's implement our model.\n",
    "We will take a MAGNN for this data. It has many disadvantages:\n",
    "- it can learn from multiple graphs simultaneously;\n",
    "- it can model long-range dependencies that many common GNN cannot.\n",
    "\n",
    "But there is also a disadvantage of the model: it's computationally expensive when we use datasets with a large number of nodes and edges. You will see it a bit later.\n",
    "Let's look at the architecture closer.\n",
    "\n",
    "---\n",
    "\n",
    "## MAGNN workflow ##\n",
    "Specifically, MAGNN first applies type-specific linear transformations to project heterogeneous node attributes, with possibly unequal dimensions for different node\n",
    "types, to the same latent vector space. Next, MAGNN applies intra-metapath aggregation with the attention mechanism for every metapath.\n",
    "During this intrametapath aggregation, each target node extracts and combines information from the metapath instances connecting the node with its metapath-based neighbors.\n",
    "In this way, MAGNN captures the structural and semantic information of heterogeneous graphs from both neighbor nodes and the metapath context in between.\n",
    "Following intra-metapath aggregation, MAGNN further conducts intermetapath aggregation using the attention mechanism to fuse latent vectors obtained from\n",
    "multiple metapaths into final node embeddings. By integrating multiple metapaths, our model can learn the comprehensive semantics ingrained in the heterogeneous graph.\n",
    "\n",
    "MAGNN is constructed by three major components: node content transformation, intra-metapath aggregation, and inter-metapath aggregation.\n",
    "Here's an illustration of the embedding generation of a single node:\n",
    "![](images/workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:21:46.624233500Z",
     "start_time": "2023-12-12T15:21:46.606358200Z"
    }
   },
   "outputs": [],
   "source": [
    "class GraphAttention(nn.Module):\n",
    "    \"\"\"Attention layer for encoded metapaths\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim: int, num_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        weighted, _ = self.attention(queries, keys, values)\n",
    "        return weighted\n",
    "\n",
    "\n",
    "class MetapathEncoder(nn.Module):\n",
    "    \"\"\"Inter-metapath aggregator\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, metapath):\n",
    "        x = torch.mean(metapath, dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class MAGNN(nn.Module):\n",
    "    \"\"\"MAGNN architecture, but simplified for only 1 metapath per node type (due to computing powers limitation)\"\"\"\n",
    "\n",
    "    def __init__(self, num_user_features: int, num_movie_features: int, hidden_dim: int, out_dim: int, num_heads: int, dropout: float, batch_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.num_user_features = num_user_features\n",
    "        self.num_movie_features = num_movie_features\n",
    "\n",
    "        self.user_feature_encoder = nn.Linear(self.num_user_features, hidden_dim)\n",
    "        self.movie_feature_encoder = nn.Linear(self.num_movie_features, hidden_dim)\n",
    "\n",
    "        self.user_metapath_encoder = MetapathEncoder(hidden_dim, hidden_dim)\n",
    "        self.user_metapath_attention = GraphAttention(hidden_dim, num_heads, dropout)\n",
    "\n",
    "        self.movie_metapath_encoder = MetapathEncoder(hidden_dim, hidden_dim)\n",
    "        self.movie_metapath_attention = GraphAttention(hidden_dim, num_heads, dropout)\n",
    "\n",
    "        self.user_node_embedding = nn.Linear(hidden_dim, out_dim)\n",
    "        self.user_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.movie_node_embedding = nn.Linear(hidden_dim, out_dim)  #, dropout)  # TODO тут ведь не Dropout, а bias\n",
    "        self.movie_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.recommender = nn.Linear(2 * out_dim, 1)\n",
    "\n",
    "    def forward(self, _, user_metapaths1, user_metapaths2, user_metapaths3, movie_metapaths1, movie_metapaths2, movie_metapaths3):\n",
    "        \"\"\"\n",
    "        Forward propagation of the model.\n",
    "        \n",
    "        You can find the pseudocode of the function in the paper that was mentioned in the beginning.\n",
    "        \"\"\"\n",
    "        user_metapath_instance = torch.cat([\n",
    "            self.user_feature_encoder(user_metapaths1),\n",
    "            self.movie_feature_encoder(user_metapaths2),\n",
    "            self.user_feature_encoder(user_metapaths3)\n",
    "        ], dim=1)\n",
    "        user_aggregated_metapath = F.tanh(self.user_metapath_encoder(user_metapath_instance))\n",
    "        user_aggregated_metapath = self.user_metapath_attention(user_aggregated_metapath)\n",
    "\n",
    "        movie_metapath_instance = torch.cat([\n",
    "            self.movie_feature_encoder(movie_metapaths1),\n",
    "            self.user_feature_encoder(movie_metapaths2),\n",
    "            self.movie_feature_encoder(movie_metapaths3)\n",
    "        ], dim=1)\n",
    "        movie_aggregated_metapath = F.tanh(self.movie_metapath_encoder(movie_metapath_instance))\n",
    "        movie_aggregated_metapath = self.movie_metapath_attention(movie_aggregated_metapath)\n",
    "\n",
    "        user_embed = self.user_dropout(F.sigmoid(self.user_node_embedding(user_aggregated_metapath)))\n",
    "        movie_embed = self.movie_dropout(F.sigmoid(self.movie_node_embedding(movie_aggregated_metapath)))\n",
    "\n",
    "        return F.sigmoid(self.recommender(torch.cat([user_embed, movie_embed], dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T12:00:11.337977400Z",
     "start_time": "2023-12-12T12:00:11.229642700Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MAGNN(num_user_features, num_movie_features, 128, 128, 8, 0.2, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To train the model we will use BCELoss and Adam optimizer. All settings you may see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T12:00:11.370888900Z",
     "start_time": "2023-12-12T12:00:11.271285800Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5, weight_decay=2e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T12:00:11.373880500Z",
     "start_time": "2023-12-12T12:00:11.275785600Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch: int):\n",
    "    losses = []\n",
    "    accs = []\n",
    "\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, f\"Epoch {epoch}\")\n",
    "    for batch in progress_bar:\n",
    "        edge, label, umetapath1, umetapath2, umetapath3, mmetapath1, mmetapath2, mmetapath3 = batch\n",
    "        edge, label, umetapath1, umetapath2, umetapath3, mmetapath1, mmetapath2, mmetapath3 = \\\n",
    "            edge.to(device), label.to(device), umetapath1.to(device), umetapath2.to(device), umetapath3.to(\n",
    "                device), mmetapath1.to(device), mmetapath2.to(device), mmetapath3.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(edge, umetapath1, umetapath2, umetapath3, mmetapath1, mmetapath2, mmetapath3)\n",
    "\n",
    "        pred = (out > 0.5)\n",
    "\n",
    "        loss = criterion(out, label)\n",
    "        acc = (pred == label).sum() / BATCH_SIZE\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        accs.append(acc.cpu())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_description(f\"Epoch {epoch}, Loss: {np.mean(losses):.5f}, Acc: {np.mean(accs):.5f}\")\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def validate(epoch: int):\n",
    "    losses = []\n",
    "    accs = []\n",
    "\n",
    "    model.eval()\n",
    "    progress_bar = tqdm(test_loader)\n",
    "    for batch in progress_bar:\n",
    "        edge, label, umetapath1, umetapath2, umetapath3, mmetapath1, mmetapath2, mmetapath3 = batch\n",
    "        edge, label, umetapath1, umetapath2, umetapath3, mmetapath1, mmetapath2, mmetapath3 = \\\n",
    "            edge.to(device), label.to(device), umetapath1.to(device), umetapath2.to(device), umetapath3.to(\n",
    "                device), mmetapath1.to(device), mmetapath2.to(device), mmetapath3.to(device)\n",
    "\n",
    "        out = model(edge, umetapath1, umetapath2, umetapath3, mmetapath1, mmetapath2, mmetapath3)\n",
    "\n",
    "        pred = (out > 0.5)\n",
    "\n",
    "        loss = criterion(out, label)\n",
    "        acc = (pred == label).sum() / BATCH_SIZE\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        accs.append(acc.cpu())\n",
    "\n",
    "        progress_bar.set_description(f\"Validation Epoch {epoch}, Loss: {np.mean(losses):.5f}, Acc: {np.mean(accs):.5f}\")\n",
    "\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T12:00:11.375875100Z",
     "start_time": "2023-12-12T12:00:11.299568800Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T12:35:52.405457900Z",
     "start_time": "2023-12-12T12:00:11.315039900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2031 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.64710, Acc: 0.60504: 100%|██████████| 2031/2031 [00:44<00:00, 45.65it/s]\n",
      "Validation Epoch 0, Loss: 0.49078, Acc: 0.84281: 100%|██████████| 581/581 [00:17<00:00, 33.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.58588, Acc: 0.72608:  74%|███████▍  | 1508/2031 [00:30<00:10, 49.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[49], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# TODO почему тут вывод не сохраняется?\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(EPOCHS):\n\u001B[1;32m----> 8\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m train(epoch)\n\u001B[0;32m      9\u001B[0m     val_loss \u001B[38;5;241m=\u001B[39m validate(epoch)\n\u001B[0;32m     10\u001B[0m     train_losses\u001B[38;5;241m.\u001B[39mappend(train_loss)\n",
      "Cell \u001B[1;32mIn[47], line 28\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(epoch)\u001B[0m\n\u001B[0;32m     25\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     26\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 28\u001B[0m     progress_bar\u001B[38;5;241m.\u001B[39mset_description(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mmean(losses)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.5f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Acc: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mmean(accs)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.5f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mmean(losses)\n",
      "File \u001B[1;32m<__array_function__ internals>:200\u001B[0m, in \u001B[0;36mmean\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:3464\u001B[0m, in \u001B[0;36mmean\u001B[1;34m(a, axis, dtype, out, keepdims, where)\u001B[0m\n\u001B[0;32m   3461\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3462\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m mean(axis\u001B[38;5;241m=\u001B[39maxis, dtype\u001B[38;5;241m=\u001B[39mdtype, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m-> 3464\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _methods\u001B[38;5;241m.\u001B[39m_mean(a, axis\u001B[38;5;241m=\u001B[39maxis, dtype\u001B[38;5;241m=\u001B[39mdtype,\n\u001B[0;32m   3465\u001B[0m                       out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\_methods.py:165\u001B[0m, in \u001B[0;36m_mean\u001B[1;34m(a, axis, dtype, out, keepdims, where)\u001B[0m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_mean\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;241m*\u001B[39m, where\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m--> 165\u001B[0m     arr \u001B[38;5;241m=\u001B[39m asanyarray(a)\n\u001B[0;32m    167\u001B[0m     is_float16_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    169\u001B[0m     rcount \u001B[38;5;241m=\u001B[39m _count_reduce_items(arr, axis, keepdims\u001B[38;5;241m=\u001B[39mkeepdims, where\u001B[38;5;241m=\u001B[39mwhere)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_loss = float(\"inf\")\n",
    "model.to(device)\n",
    "# TODO почему тут вывод не сохраняется?\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train(epoch)\n",
    "    val_loss = validate(epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print()\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"./models/best_magnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T12:36:33.913126Z",
     "start_time": "2023-12-12T12:36:31.215498200Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../reports/figures/losses.png'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(val_losses, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation loss\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m plt\u001B[38;5;241m.\u001B[39mlegend()\n\u001B[1;32m----> 4\u001B[0m plt\u001B[38;5;241m.\u001B[39msavefig(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../reports/figures/losses.png\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      5\u001B[0m plt\u001B[38;5;241m.\u001B[39mshow()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\DataScience\\Lib\\site-packages\\matplotlib\\pyplot.py:1023\u001B[0m, in \u001B[0;36msavefig\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1020\u001B[0m \u001B[38;5;129m@_copy_docstring_and_deprecators\u001B[39m(Figure\u001B[38;5;241m.\u001B[39msavefig)\n\u001B[0;32m   1021\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msavefig\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m   1022\u001B[0m     fig \u001B[38;5;241m=\u001B[39m gcf()\n\u001B[1;32m-> 1023\u001B[0m     res \u001B[38;5;241m=\u001B[39m fig\u001B[38;5;241m.\u001B[39msavefig(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1024\u001B[0m     fig\u001B[38;5;241m.\u001B[39mcanvas\u001B[38;5;241m.\u001B[39mdraw_idle()  \u001B[38;5;66;03m# Need this if 'transparent=True', to reset colors.\u001B[39;00m\n\u001B[0;32m   1025\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\DataScience\\Lib\\site-packages\\matplotlib\\figure.py:3378\u001B[0m, in \u001B[0;36mFigure.savefig\u001B[1;34m(self, fname, transparent, **kwargs)\u001B[0m\n\u001B[0;32m   3374\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m ax \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxes:\n\u001B[0;32m   3375\u001B[0m         stack\u001B[38;5;241m.\u001B[39menter_context(\n\u001B[0;32m   3376\u001B[0m             ax\u001B[38;5;241m.\u001B[39mpatch\u001B[38;5;241m.\u001B[39m_cm_set(facecolor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnone\u001B[39m\u001B[38;5;124m'\u001B[39m, edgecolor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnone\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m-> 3378\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcanvas\u001B[38;5;241m.\u001B[39mprint_figure(fname, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\DataScience\\Lib\\site-packages\\matplotlib\\backend_bases.py:2366\u001B[0m, in \u001B[0;36mFigureCanvasBase.print_figure\u001B[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001B[0m\n\u001B[0;32m   2362\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   2363\u001B[0m     \u001B[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001B[39;00m\n\u001B[0;32m   2364\u001B[0m     \u001B[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001B[39;00m\n\u001B[0;32m   2365\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m cbook\u001B[38;5;241m.\u001B[39m_setattr_cm(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfigure, dpi\u001B[38;5;241m=\u001B[39mdpi):\n\u001B[1;32m-> 2366\u001B[0m         result \u001B[38;5;241m=\u001B[39m print_method(\n\u001B[0;32m   2367\u001B[0m             filename,\n\u001B[0;32m   2368\u001B[0m             facecolor\u001B[38;5;241m=\u001B[39mfacecolor,\n\u001B[0;32m   2369\u001B[0m             edgecolor\u001B[38;5;241m=\u001B[39medgecolor,\n\u001B[0;32m   2370\u001B[0m             orientation\u001B[38;5;241m=\u001B[39morientation,\n\u001B[0;32m   2371\u001B[0m             bbox_inches_restore\u001B[38;5;241m=\u001B[39m_bbox_inches_restore,\n\u001B[0;32m   2372\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2373\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   2374\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m bbox_inches \u001B[38;5;129;01mand\u001B[39;00m restore_bbox:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\DataScience\\Lib\\site-packages\\matplotlib\\backend_bases.py:2232\u001B[0m, in \u001B[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   2228\u001B[0m     optional_kws \u001B[38;5;241m=\u001B[39m {  \u001B[38;5;66;03m# Passed by print_figure for other renderers.\u001B[39;00m\n\u001B[0;32m   2229\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdpi\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfacecolor\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124medgecolor\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morientation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   2230\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbbox_inches_restore\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[0;32m   2231\u001B[0m     skip \u001B[38;5;241m=\u001B[39m optional_kws \u001B[38;5;241m-\u001B[39m {\u001B[38;5;241m*\u001B[39minspect\u001B[38;5;241m.\u001B[39msignature(meth)\u001B[38;5;241m.\u001B[39mparameters}\n\u001B[1;32m-> 2232\u001B[0m     print_method \u001B[38;5;241m=\u001B[39m functools\u001B[38;5;241m.\u001B[39mwraps(meth)(\u001B[38;5;28;01mlambda\u001B[39;00m \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: meth(\n\u001B[0;32m   2233\u001B[0m         \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m skip}))\n\u001B[0;32m   2234\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# Let third-parties do as they see fit.\u001B[39;00m\n\u001B[0;32m   2235\u001B[0m     print_method \u001B[38;5;241m=\u001B[39m meth\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\DataScience\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:509\u001B[0m, in \u001B[0;36mFigureCanvasAgg.print_png\u001B[1;34m(self, filename_or_obj, metadata, pil_kwargs)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprint_png\u001B[39m(\u001B[38;5;28mself\u001B[39m, filename_or_obj, \u001B[38;5;241m*\u001B[39m, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, pil_kwargs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    463\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    464\u001B[0m \u001B[38;5;124;03m    Write the figure to a PNG file.\u001B[39;00m\n\u001B[0;32m    465\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    507\u001B[0m \u001B[38;5;124;03m        *metadata*, including the default 'Software' key.\u001B[39;00m\n\u001B[0;32m    508\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 509\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_print_pil(filename_or_obj, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpng\u001B[39m\u001B[38;5;124m\"\u001B[39m, pil_kwargs, metadata)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\DataScience\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:458\u001B[0m, in \u001B[0;36mFigureCanvasAgg._print_pil\u001B[1;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001B[0m\n\u001B[0;32m    453\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    454\u001B[0m \u001B[38;5;124;03mDraw the canvas, then save it using `.image.imsave` (to which\u001B[39;00m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;124;03m*pil_kwargs* and *metadata* are forwarded).\u001B[39;00m\n\u001B[0;32m    456\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    457\u001B[0m FigureCanvasAgg\u001B[38;5;241m.\u001B[39mdraw(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m--> 458\u001B[0m mpl\u001B[38;5;241m.\u001B[39mimage\u001B[38;5;241m.\u001B[39mimsave(\n\u001B[0;32m    459\u001B[0m     filename_or_obj, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer_rgba(), \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39mfmt, origin\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mupper\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    460\u001B[0m     dpi\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfigure\u001B[38;5;241m.\u001B[39mdpi, metadata\u001B[38;5;241m=\u001B[39mmetadata, pil_kwargs\u001B[38;5;241m=\u001B[39mpil_kwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\DataScience\\Lib\\site-packages\\matplotlib\\image.py:1689\u001B[0m, in \u001B[0;36mimsave\u001B[1;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001B[0m\n\u001B[0;32m   1687\u001B[0m pil_kwargs\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mformat\u001B[39m)\n\u001B[0;32m   1688\u001B[0m pil_kwargs\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdpi\u001B[39m\u001B[38;5;124m\"\u001B[39m, (dpi, dpi))\n\u001B[1;32m-> 1689\u001B[0m image\u001B[38;5;241m.\u001B[39msave(fname, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpil_kwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\DataScience\\Lib\\site-packages\\PIL\\Image.py:2428\u001B[0m, in \u001B[0;36mImage.save\u001B[1;34m(self, fp, format, **params)\u001B[0m\n\u001B[0;32m   2426\u001B[0m         fp \u001B[38;5;241m=\u001B[39m builtins\u001B[38;5;241m.\u001B[39mopen(filename, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr+b\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   2427\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2428\u001B[0m         fp \u001B[38;5;241m=\u001B[39m builtins\u001B[38;5;241m.\u001B[39mopen(filename, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw+b\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   2430\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   2431\u001B[0m     save_handler(\u001B[38;5;28mself\u001B[39m, fp, filename)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../reports/figures/losses.png'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label=\"Train loss\")\n",
    "plt.plot(val_losses, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"figures/losses.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Well done! The traing is over and the result is stored in the file. Now let's check how it works."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-12T12:38:03.763140200Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MAGNN(num_user_features, num_movie_features, 128, 128, 8, 0.2, BATCH_SIZE)\n",
    "model.load_state_dict(torch.load(\"./models/best_magnn.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-12T12:38:03.764137800Z"
    }
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "maps = []\n",
    "accs = []\n",
    "rocs = []\n",
    "\n",
    "for batch in tqdm(test_loader):\n",
    "    edge, label, umetapath1, umetapath2, umetapath3, mmetapath1, mmetapath2, mmetapath3 = batch\n",
    "    edge, label, umetapath1, umetapath2, umetapath3, mmetapath1, mmetapath2, mmetapath3 = \\\n",
    "        edge.to(device), label.to(device), umetapath1.to(device), umetapath2.to(device), umetapath3.to(\n",
    "            device), mmetapath1.to(device), mmetapath2.to(device), mmetapath3.to(device)\n",
    "\n",
    "    out = model(edge, umetapath1, umetapath2, umetapath3, mmetapath1, mmetapath2, mmetapath3)\n",
    "    pred = (out > 0.5)\n",
    "    acc = (pred == label.reshape(-1, 1)).sum() / BATCH_SIZE\n",
    "\n",
    "    accs.append(acc.cpu())\n",
    "    maps.append(average_precision_score(label.reshape(-1, 1).cpu().detach().numpy(), out.cpu().detach().numpy()))\n",
    "    rocs.append(roc_auc_score(label.reshape(-1, 1).cpu().detach().numpy(), out.cpu().detach().numpy()))\n",
    "\n",
    "print(f\"mAP: {np.mean(maps)}\")\n",
    "print(f\"Accuracy: {np.mean(accs)}\")\n",
    "print(f\"ROC-AUC score: {np.mean(rocs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's it! Try to use your data and find some music for you own:)\n",
    "If you have any questions, or you want to add something to this tutorial, don't be afraid to text us!"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
